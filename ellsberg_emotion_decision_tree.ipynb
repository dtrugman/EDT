{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import entropy\n",
    "\n",
    "df = pd.read_csv('data/data_ellsberg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ['ID', 'participant_id']\n",
    "\n",
    "dt_features = ['Openess','Conciensiousness','Extroversion','Agreability','Stability','Locus']\n",
    "\n",
    "x_labels = []\n",
    "emotions = ['Hopeful','Curiosity','Enlightenment','Thrilled','Anticipatory','Satisfied' ]\n",
    "kmeans_features = []\n",
    "for x in ['1A', '1B', '2A', '2B']:\n",
    "    kmeans_features += [f'{emotion}{x}' for emotion in emotions]\n",
    "    x_labels.append([f'{emotion[:4]}{x}' for emotion in emotions])\n",
    "\n",
    "label = ['DAA']\n",
    "\n",
    "df = df[ids + kmeans_features + dt_features + label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_results(labels, centeroids, plot_graph=False):\n",
    "    k = len(centeroids) # number of clusters\n",
    "\n",
    "    print('labels:', labels, '\\n') \n",
    "    print('centroids:')\n",
    "    for i in range(k):\n",
    "        print(i,':',centeroids[i,:],'\\n')\n",
    "\n",
    "    if plot_graph:\n",
    "        games_count = len(x_labels)\n",
    "        labels_count = len(emotions)\n",
    "        _, axs = plt.subplots(games_count, figsize=(10, 10))\n",
    "        for ig in range(games_count):\n",
    "            graph_labels = x_labels[ig]\n",
    "            graph_centers = centeroids[:,ig*labels_count:(ig+1)*labels_count]\n",
    "            for ik in range(k):\n",
    "                axs[ig].plot(graph_labels, graph_centers[ik,:], label=f'Cluster #{ik}')\n",
    "            axs[ig].legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 # Optimal value according to 'daa_kmeans.ipynb' was K=4, but silhouette difference didn't justify working with 2 extra clusters\n",
    "R = 0 # Set to 'None' to randomise KMeans search\n",
    "\n",
    "def build_kmeans(points, k, r):\n",
    "    clusterer = KMeans(n_clusters=k, n_init='auto', random_state=r)\n",
    "    preds = clusterer.fit_predict(points)\n",
    "    return clusterer, preds\n",
    "\n",
    "clusterer, _ = build_kmeans(df[kmeans_features], K, R)\n",
    "labels = clusterer.labels_\n",
    "centeroids = clusterer.cluster_centers_\n",
    "\n",
    "show_results(labels, centeroids, plot_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = labels\n",
    "df = df.drop(kmeans_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ClusterTable:\n",
    "\n",
    "    EPS = 1e-5\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.__df = df\n",
    "\n",
    "        self.__df_counts = self.__build_matrix_of_counts(self.__df)\n",
    "        self.__df_p = self.__build_matrix_of_p(self.__df_counts)\n",
    "\n",
    "        self.__daa_counts = self.__df_counts.sum(axis=1)\n",
    "        self.__daa_p = self.__counts_to_p(self.__daa_counts)\n",
    "\n",
    "        self.__cluster_counts = self.__df_counts.sum(axis=0)\n",
    "        self.__cluster_p = self.__counts_to_p(self.__cluster_counts)\n",
    "\n",
    "    @staticmethod\n",
    "    def __build_matrix_of_counts(df):\n",
    "        return pd.crosstab(index=df['DAA'], columns=df['Cluster'])\n",
    "\n",
    "    @staticmethod\n",
    "    def __build_matrix_of_p(df_count):\n",
    "        columns = range(K)\n",
    "        index = [0,1] # DAA values\n",
    "        odds = df_count.div(df_count.sum(axis=1).sum(axis=0), axis=0)\n",
    "        return odds.reindex(index=index, columns=columns, fill_value=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def __counts_to_p(occurences):\n",
    "        return occurences.div(occurences.sum())\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''\n",
    "Samples:               {self.len}\n",
    "DAA (X) total:         {self.__daa_counts.tolist()}\n",
    "DAA (X) p:             {self.__daa_p.tolist()}\n",
    "DAA (X) entropy:       {self.__daa_entropy}\n",
    "Cluster (Y) total:     {self.__cluster_counts.tolist()}\n",
    "Cluster (Y) p:         {self.__cluster_p.tolist()}\n",
    "Cluster (Y) entropy:   {self.__cluster_entropy}\n",
    "Joint entropy:         {self.__joint_entropy}\n",
    "Mutual info:           {self.__mutual_information}\n",
    "Symmetric uncertainty: {self.symmetric_uncertainty}\n",
    "KL-divergence:         {self.kl_divergence}\n",
    "Total var distance:    {self.total_variation_distance}\n",
    "{self.__df_counts.to_string()}\n",
    "{self.__df_p.to_string()}\n",
    "'''\n",
    "\n",
    "    @property\n",
    "    def len(self):\n",
    "        return len(self.__df)\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self.__df\n",
    "\n",
    "    @property\n",
    "    def df_p(self):\n",
    "        return self.__df_p\n",
    "\n",
    "    @property\n",
    "    def __cluster_p_for_non_daa(self):\n",
    "        return self.__df_p.loc[0]\n",
    "\n",
    "    @property\n",
    "    def __cluster_p_for_daa(self):\n",
    "        return self.__df_p.loc[1]\n",
    "\n",
    "    @property\n",
    "    def __daa_entropy(self):\n",
    "        return entropy(self.__daa_p + ClusterTable.EPS, base=2)\n",
    "\n",
    "    @property\n",
    "    def __cluster_entropy(self):\n",
    "        return entropy(self.__cluster_p + ClusterTable.EPS, base=2)\n",
    "\n",
    "    @property\n",
    "    def __joint_entropy(self):\n",
    "        joint_p = np.concatenate([self.__cluster_p_for_daa, self.__cluster_p_for_non_daa])\n",
    "        return entropy(joint_p + ClusterTable.EPS, base=2)\n",
    "\n",
    "    @property\n",
    "    def __mutual_information(self):\n",
    "        return self.__daa_entropy + self.__cluster_entropy - self.__joint_entropy\n",
    "\n",
    "    @property\n",
    "    def symmetric_uncertainty(self):\n",
    "        return (2 * self.__mutual_information) / (self.__daa_entropy + self.__cluster_entropy)\n",
    "\n",
    "    @property\n",
    "    def total_variation_distance(self):\n",
    "        return 0.5 * sum(abs(self.__cluster_p_for_daa - self.__cluster_p_for_non_daa))\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        pk = self.__cluster_p_for_daa + ClusterTable.EPS\n",
    "        qk = self.__cluster_p_for_non_daa + ClusterTable.EPS\n",
    "        pq = entropy(pk, qk=qk, base=2)\n",
    "        qp = entropy(qk, qk=pk, base=2)\n",
    "        return sum([pq, qp]) / 2\n",
    "\n",
    "    @property\n",
    "    def metric(self):\n",
    "        return self.symmetric_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    ID = 0\n",
    "\n",
    "    def __init__(self, df, min_gain, min_samples_leaf):\n",
    "        self.__log = logging.getLogger('dt')\n",
    "\n",
    "        self.__id = DecisionNode.ID\n",
    "        DecisionNode.ID += 1\n",
    "        self.__ct = ClusterTable(df)\n",
    "\n",
    "        self.__min_samples_leaf = min_samples_leaf\n",
    "        self.__min_samples_split = 3 * self.__min_samples_leaf\n",
    "        self.__min_gain = min_gain\n",
    "\n",
    "        self.__param = None\n",
    "        self.__cutoff = None\n",
    "        self.__gain = None\n",
    "        self.__left = None\n",
    "        self.__right = None\n",
    "\n",
    "        self.__fit()\n",
    "    \n",
    "    @property\n",
    "    def id(self):\n",
    "        return self.__id\n",
    "\n",
    "    @property\n",
    "    def ct(self):\n",
    "        return self.__ct\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.__left == None and self.__right == None\n",
    "\n",
    "    @property\n",
    "    def param(self):\n",
    "        return self.__param\n",
    "    \n",
    "    @property\n",
    "    def cutoff(self):\n",
    "        return self.__cutoff\n",
    "\n",
    "    @property\n",
    "    def left(self):\n",
    "        return self.__left\n",
    "\n",
    "    @property\n",
    "    def right(self):\n",
    "        return self.__right\n",
    "\n",
    "    def __find_cutoff(self, param, cutoff):\n",
    "        left_df = self.__ct.df[self.__ct.df[param] <= cutoff]\n",
    "        right_df = self.__ct.df[self.__ct.df[param] > cutoff]\n",
    "        return left_df, right_df\n",
    "        \n",
    "    def __find_cutoff_gain(self, current, param, cutoff):\n",
    "        left_df, right_df = self.__find_cutoff(param, cutoff)\n",
    "\n",
    "        left_len = len(left_df)\n",
    "        right_len = len(right_df)\n",
    "\n",
    "        if left_len < self.__min_samples_leaf or right_len < self.__min_samples_leaf:\n",
    "            self.__log.info(f'\\t\\t\\tNot enough samples in leaf: {left_len}, {right_len}')\n",
    "            return 0\n",
    "\n",
    "        left_ct = ClusterTable(left_df)\n",
    "        self.__log.debug('\\t\\t\\t' + '\\n\\t\\t\\t'.join(left_ct.df_p.to_string().split('\\n')))\n",
    "\n",
    "        left_metric = left_ct.metric\n",
    "        self.__log.info(f'\\t\\t\\tL: {left_metric}')\n",
    "\n",
    "        right_ct = ClusterTable(right_df)\n",
    "        self.__log.debug('\\t\\t\\t' + '\\n\\t\\t\\t'.join(right_ct.df_p.to_string().split('\\n')))\n",
    "\n",
    "        right_metric = right_ct.metric\n",
    "        self.__log.info(f'\\t\\t\\tR: {right_metric}')\n",
    "\n",
    "        return max(left_metric - current, right_metric - current)\n",
    "\n",
    "    def __find_best_cutoff_index(self, current, param):\n",
    "        best_cutoff = best_gain = None\n",
    "\n",
    "        values = sorted(self.__ct.df[param].unique())\n",
    "        cutoffs = [(values[i] + values[i+1]) / 2 for i in range(len(values) - 1)]\n",
    "        self.__log.info(f'\\t{values}')\n",
    "        np.random.shuffle(cutoffs)\n",
    "        for cutoff in cutoffs:\n",
    "            self.__log.info(f'\\t\\t{cutoff}')\n",
    "            gain = self.__find_cutoff_gain(current, param, cutoff)\n",
    "            self.__log.info(f'\\t\\t\\tGain: {gain}')\n",
    "            if not best_gain or gain > best_gain:\n",
    "                best_cutoff, best_gain = cutoff, gain\n",
    "\n",
    "        return best_cutoff, best_gain\n",
    "\n",
    "    def __find_best_cutoff(self, current):\n",
    "        best_param = best_cutoff = best_gain = None\n",
    "\n",
    "        params = dt_features.copy()\n",
    "        np.random.shuffle(params)\n",
    "        for param in params:\n",
    "            self.__log.info(f'\\t{param}')\n",
    "            cutoff, gain = self.__find_best_cutoff_index(current, param)\n",
    "            self.__log.info(f'\\tBest gain for {param}: {gain}')\n",
    "\n",
    "            if not gain:\n",
    "                continue\n",
    "\n",
    "            if not best_gain or gain > best_gain:\n",
    "                best_param, best_cutoff, best_gain = param, cutoff, gain\n",
    "\n",
    "        return best_param, best_cutoff, best_gain \n",
    "\n",
    "    def __fit(self):\n",
    "        current = self.__ct.metric\n",
    "        self.__log.warning(f'Node #{self.__id} [{current}]')\n",
    "\n",
    "        if self.ct.len < self.__min_samples_split:\n",
    "            self.__log.warning(f'\\tNot splitting node, not enough samples')\n",
    "            return\n",
    "\n",
    "        self.__param, self.__cutoff, self.__gain = self.__find_best_cutoff(current)\n",
    "        if not self.__gain or self.__gain < self.__min_gain:\n",
    "            self.__log.warning('\\tNot splitting node, gain too low')\n",
    "            return\n",
    "\n",
    "        left_df, right_df = self.__find_cutoff(self.__param, self.__cutoff)\n",
    "\n",
    "        self.__log.warning(f'\\tBest gain: {self.__gain}\\n')\n",
    "        self.__log.warning('\\tLeft:\\t' + '\\n\\t'.join(str(ClusterTable(left_df)).split('\\n')))\n",
    "        self.__log.warning('\\tRight:\\t' + '\\n\\t'.join(str(ClusterTable(right_df)).split('\\n')))\n",
    "\n",
    "        self.__left = DecisionNode(left_df,\n",
    "                                   min_gain=self.__min_gain,\n",
    "                                   min_samples_leaf=self.__min_samples_leaf)\n",
    "        self.__right = DecisionNode(right_df,\n",
    "                                    min_gain=self.__min_gain,\n",
    "                                    min_samples_leaf=self.__min_samples_leaf)\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, df, min_gain, min_samples_leaf):\n",
    "        self.__root = DecisionNode(df,\n",
    "                                   min_gain=min_gain,\n",
    "                                   min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        return self.__root\n",
    "\n",
    "logger = logging.getLogger('dt')\n",
    "logger.setLevel('WARNING')\n",
    "\n",
    "np.random.default_rng(seed=1337)\n",
    "dt = DecisionTree(df, min_gain=0.01, min_samples_leaf=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPrinter:\n",
    "\n",
    "    def __init__(self, dt):\n",
    "        self.__dt = dt\n",
    "\n",
    "    def __node_children(self, node, left):\n",
    "        label = 'xlabel' if left else 'label'\n",
    "\n",
    "        operator = '<=' if left else '>'\n",
    "        criteria = f'{node.param} {operator} {node.cutoff}'\n",
    "\n",
    "        child = node.left if left else node.right\n",
    "        child_label = self.__node_label(child)\n",
    "        child_node = f'{child.id} [label=\"{child_label}\",shape=box,style=filled,color=\".7 .3 1.\"];'\n",
    "        child_edge = f'{node.id} -> {child.id} [{label}=\"{criteria}\"];'\n",
    "        return '\\n'.join([child_node, child_edge])\n",
    "\n",
    "    def __node_label(self, node):\n",
    "        len = node.ct.len\n",
    "        su = node.ct.metric\n",
    "        values = node.ct.df['DAA'].value_counts().to_list()\n",
    "        return f'id = {node.id}\\\\n' + \\\n",
    "               f'samples = {len}\\\\n' + \\\n",
    "               f'metric = {su:.5f}\\\\n' + \\\n",
    "               f'values = {values}'\n",
    "\n",
    "    def __node_content(self, node):\n",
    "        if node.is_leaf:\n",
    "            return ''\n",
    "\n",
    "        return '\\n'.join([\n",
    "            self.__node_children(node, left=True),\n",
    "            self.__node_content(node.left),\n",
    "            self.__node_children(node, left=False),\n",
    "            self.__node_content(node.right)\n",
    "        ])\n",
    "\n",
    "    def print(self):\n",
    "        node_label = self.__node_label(self.__dt.root)\n",
    "        node_content = self.__node_content(self.__dt.root)\n",
    "\n",
    "        return '\\n'.join([\n",
    "            'digraph BST {',\n",
    "            'node [fontname=\"Tahoma\"]',\n",
    "            f'{self.__dt.root.id} [label=\"{node_label}\",shape=box,style=filled,color=\".6 .2 1.\"];',\n",
    "            node_content,\n",
    "            '}'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write report: Generate a DOT file and a PNG of the emotion decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "report_basename = 'ellsberg_emotion_decision_tree'\n",
    "\n",
    "report_dot = f'{report_basename}.dot'\n",
    "with open(report_dot, 'w') as fout:\n",
    "    dot_output = DotPrinter(dt)\n",
    "    fout.write(dot_output.print())\n",
    "\n",
    "report_png = f'{report_basename}.png'\n",
    "subprocess.check_call(['dot', report_dot, '-Tpng', f'-o{report_png}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
